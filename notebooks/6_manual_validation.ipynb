{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dab2c783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "‚úÖ Setup Complete. Root: /home/juansebas7ian/deep-agents-from-scratch\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SETUP\n",
    "# =============================================================================\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "load_dotenv()\n",
    "\n",
    "print(f\"‚úÖ Setup Complete. Root: {os.path.abspath('..')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "110aac71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 1. Testing Write ---\n",
      "\n",
      "‚ùå EXECUTION ERROR: 1 validation error for save_task\n",
      "description\n",
      "  Field required [type=missing, input_value={'user_id': 'user_val_tes...lidate DynamoDB Client'}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.12/v/missing\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# PASO 1: VALIDAR EL \"M√öSCULO\" (DynamoDB)\n",
    "# =============================================================================\n",
    "# Validamos neuro_agent/src/dynamo_client.py\n",
    "\n",
    "try:\n",
    "    from neuro_agent.src.shared.tools import save_task, get_context\n",
    "    \n",
    "    USER_TEST_ID = \"user_val_test\"\n",
    "    TASK_DESC = \"Unit Test Task: Validate DynamoDB Client\"\n",
    "    \n",
    "    print(\"\\n--- 1. Testing Write ---\")\n",
    "    # save_task is a @tool, so we invoke it or call it directly if possible.\n",
    "    # It returns a string.\n",
    "    try:\n",
    "        if hasattr(save_task, 'invoke'):\n",
    "             # If it's a StructuredTool\n",
    "             write_result = save_task.invoke({'user_id': USER_TEST_ID, 'task_description': TASK_DESC})\n",
    "        else:\n",
    "             write_result = save_task(USER_TEST_ID, TASK_DESC)\n",
    "    except Exception as ie:\n",
    "        # Fallback if arguments are positional but tool expects kwarg dictionary for invoke\n",
    "         write_result = save_task.invoke({'user_id': USER_TEST_ID, 'task_description': TASK_DESC})\n",
    "\n",
    "    print(f\"Write Result: {write_result}\")\n",
    "    \n",
    "    print(\"\\n--- 2. Testing Read ---\")\n",
    "    context = get_context(USER_TEST_ID)\n",
    "    print(f\"Context Fetched: {context}\")\n",
    "    \n",
    "    # Assert\n",
    "    todos = context.get(\"todos\", [])\n",
    "    found = any(t.get('description') == TASK_DESC for t in todos)\n",
    "    \n",
    "    if found:\n",
    "        print(\"\\n‚úÖ STEP 1 SUCCESS: Task written and retrieved from DynamoDB.\")\n",
    "    else:\n",
    "        print(\"\\n‚ùå STEP 1 FAILED: Task not found in fetched context.\")\n",
    "        \n",
    "except ImportError as e:\n",
    "    print(f\"\\n‚ùå IMPORT ERROR: {e}\")\n",
    "    print(\"Ensure neuro_agent/src/dynamo_client.py exists and is in the python path.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå EXECUTION ERROR: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7a0a3b26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Testing Lambda Handler ---\n",
      "Invoking lambda_handler with event: {'user_id': 'user_val_test', 'explicit_instructions': 'Execute unit test task: Buy milk via Lambda'}\n",
      "Response: {'statusCode': 200, 'body': \"<thinking> The user's request to execute a unit test task for buying milk via Lambda is not directly achievable with the provided tool. There is no tool available to execute Lambda functions or to perform unit testing for tasks like buying milk. I need to inform the user that I cannot fulfill this request with the current set of tools. </thinking>\\nNo suitable tool is available to execute the requested unit test task.\"}\n",
      "\n",
      "‚úÖ STEP 2 SUCCESS: Executor Handler executed successfully and returned 200.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# PASO 2: VALIDAR EL \"AGENTE IDIOTA\" (Executor Handler)\n",
    "# =============================================================================\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add neuro_agent to path to allow 'from src.dynamo_client import ...' inside lambda_executor.py\n",
    "project_root = os.path.abspath('..')\n",
    "neuro_agent_dir = os.path.join(project_root, 'neuro_agent')\n",
    "\n",
    "if neuro_agent_dir not in sys.path:\n",
    "    sys.path.append(neuro_agent_dir)\n",
    "    print(f\"Added {neuro_agent_dir} to sys.path\")\n",
    "\n",
    "try:\n",
    "    # Import the handler. 'neuro_agent/deploy/lambda_executor.py' imports 'src.dynamo_client'.\n",
    "    # 'src' is in 'neuro_agent'. So 'import src' works inside 'lambda_executor.py' because we added 'neuro_agent' to sys.path.\n",
    "    # However, 'deploy' is also in 'neuro_agent'.\n",
    "    # So 'from deploy.lambda_executor import lambda_handler' should work if we import relative to 'neuro_agent'.\n",
    "    \n",
    "    # Let's try importing as if we are in 'neuro_agent'.\n",
    "    from deploy.lambda_executor import lambda_handler\n",
    "    \n",
    "    print(\"\\n--- Testing Lambda Handler ---\")\n",
    "    USER_TEST_ID = \"user_val_test\"\n",
    "    INSTRUCTION = \"Execute unit test task: Buy milk via Lambda\"\n",
    "    \n",
    "    mock_event = {\n",
    "        \"user_id\": USER_TEST_ID,\n",
    "        \"explicit_instructions\": INSTRUCTION\n",
    "    }\n",
    "    \n",
    "    print(f\"Invoking lambda_handler with event: {mock_event}\")\n",
    "    # Invoke synchronously\n",
    "    response = lambda_handler(mock_event, None)\n",
    "    print(f\"Response: {response}\")\n",
    "    \n",
    "    if response.get('statusCode') == 200:\n",
    "        print(\"\\n‚úÖ STEP 2 SUCCESS: Executor Handler executed successfully and returned 200.\")\n",
    "    else:\n",
    "        print(f\"\\n‚ùå STEP 2 FAILED: Response code {response.get('statusCode')}\")\n",
    "\n",
    "except ImportError as e:\n",
    "    print(f\"\\n‚ùå IMPORT ERROR: {e}\")\n",
    "    print(\"Ensure 'neuro_agent' directory is correctly added to sys.path\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå EXECUTION ERROR: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "step3_scrape",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 3. Testing Scrape Tool ---\n",
      "Scraping https://example.com...\n",
      "Result Snippet: Example Domain\n",
      "\n",
      "Example Domain\n",
      "==============\n",
      "\n",
      "This domain is for use in documentation examples without needing permission. Avoid use in operations.\n",
      "\n",
      "[Learn more](https://iana.org/domains/example)...\n",
      "\n",
      "‚úÖ STEP 3 SUCCESS: Scrape tool fetched content correctly.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# PASO 3: VALIDAR HERRAMIENTAS INDIVIDUALES (Scrape)\n",
    "# =============================================================================\n",
    "from neuro_agent.src.shared.tools import scrape_webpage\n",
    "\n",
    "print(\"\\n--- 3. Testing Scrape Tool ---\")\n",
    "URL = \"https://example.com\"\n",
    "print(f\"Scraping {URL}...\")\n",
    "try:\n",
    "    # FIX: Use .invoke() for StructuredTools\n",
    "    result = scrape_webpage.invoke({\"url\": URL})\n",
    "    print(f\"Result Snippet: {result[:200]}...\")\n",
    "\n",
    "    if \"Example Domain\" in result:\n",
    "        print(\"\\n‚úÖ STEP 3 SUCCESS: Scrape tool fetched content correctly.\")\n",
    "    else:\n",
    "        print(\"\\n‚ùå STEP 3 FAILED: Content validation failed.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå STEP 3 ERROR: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "step4_supervisor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 4. Testing Supervisor Agent Flow ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_30035/2935532948.py:19: LangGraphDeprecatedSinceV10: create_react_agent has been moved to `langchain.agents`. Please update your import to `from langchain.agents import create_agent`. Deprecated in LangGraph V1.0 to be removed in V2.0.\n",
      "  agent = create_react_agent(llm, tools=tools)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invoking Agent with query: 'Give me a brief overview of Model Context Protocol (MCP) using a web search. Read the docs if possible.'\n",
      "Observing tool calls... (Expecting: tavily_search -> scrape_webpage)\n",
      "üîß Tool Call: tavily_search\n",
      "üîß Tool Call: scrape_webpage\n",
      "ü§ñ Answer Snippet: <thinking> It appears that the `scrape_webpage` tool encountered a \"403 Forbidden\" error, which mean...\n",
      "\n",
      "--- Validation Report ---\n",
      "‚úÖ STEP 4 SUCCESS: Agent used Search AND Scrape!\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# PASO 4: VALIDAR EL \"CEREBRO\" (Supervisor + Tools Agnostiocas)\n",
    "# =============================================================================\n",
    "from langchain_aws import ChatBedrockConverse\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_core.messages import HumanMessage\n",
    "from neuro_agent.src.shared.tools import (\n",
    "    tavily_search, think_tool, ls, read_file, write_file, \n",
    "    write_todos, read_todos, get_today_str, scrape_webpage\n",
    ")\n",
    "\n",
    "print(\"\\n--- 4. Testing Supervisor Agent Flow ---\")\n",
    "\n",
    "# 1. Setup Model & Tools\n",
    "llm = ChatBedrockConverse(model=\"us.amazon.nova-pro-v1:0\", region_name=\"us-east-1\", temperature=0.0)\n",
    "tools = [ls, read_file, write_file, write_todos, read_todos, think_tool, tavily_search, scrape_webpage]\n",
    "\n",
    "# 2. Create Agent\n",
    "agent = create_react_agent(llm, tools=tools)\n",
    "\n",
    "# 3. Run Query\n",
    "USER_QUERY = \"Give me a brief overview of Model Context Protocol (MCP) using a web search. Read the docs if possible.\"\n",
    "print(f\"Invoking Agent with query: '{USER_QUERY}'\")\n",
    "print(\"Observing tool calls... (Expecting: tavily_search -> scrape_webpage)\")\n",
    "\n",
    "try:\n",
    "    stream = agent.stream(\n",
    "        {\"messages\": [HumanMessage(content=USER_QUERY)]},\n",
    "        stream_mode=\"values\"\n",
    "    )\n",
    "\n",
    "    tool_calls_observed = set()\n",
    "\n",
    "    for event in stream:\n",
    "        if \"messages\" in event:\n",
    "            last_msg = event[\"messages\"][-1]\n",
    "            \n",
    "            # Check for Tool Calls\n",
    "            tool_calls = getattr(last_msg, 'tool_calls', [])\n",
    "            if tool_calls:\n",
    "                for tc in tool_calls:\n",
    "                    tool_name = tc['name']\n",
    "                    print(f\"üîß Tool Call: {tool_name}\")\n",
    "                    tool_calls_observed.add(tool_name)\n",
    "            \n",
    "            # Check for Content\n",
    "            if hasattr(last_msg, 'content') and last_msg.content and not tool_calls:\n",
    "                 if last_msg.type == 'ai':\n",
    "                    print(f\"ü§ñ Answer Snippet: {last_msg.content[:100]}...\")\n",
    "\n",
    "    # 4. Assertions\n",
    "    print(\"\\n--- Validation Report ---\")\n",
    "    if 'tavily_search' in tool_calls_observed and 'scrape_webpage' in tool_calls_observed:\n",
    "        print(\"‚úÖ STEP 4 SUCCESS: Agent used Search AND Scrape!\")\n",
    "    elif 'tavily_search' in tool_calls_observed:\n",
    "        print(\"‚ö†Ô∏è STEP 4 WARNING: Agent searched but did NOT scrape.\")\n",
    "    else:\n",
    "        print(f\"‚ùå STEP 4 FAILURE: Missing critical tools. Observed: {tool_calls_observed}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå STEP 4 ERROR: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a49dcd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
